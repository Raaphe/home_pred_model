{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Value Prediction Model\n",
    "---\n",
    "\n",
    "This model will use the usa-real-estate dataset of data scraped from `realtor.com`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import utils;\n",
    "from sklearn.preprocessing import LabelEncoder;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "import torch;\n",
    "from torch import nn;\n",
    "from torch.utils.data import DataLoader, Dataset;\n",
    "from tqdm.auto import tqdm;\n",
    "from typing import Tuple\n",
    "import torch.onnx;\n",
    "\n",
    "DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\";\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"realtor-data.zip.csv\");\n",
    "df.head(5), df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "---\n",
    "\n",
    "in this section, I have a lot of bad data, I will have two renditions of my dataframe, one with missing data replaced by the median, one replaced by the mean, and one with missing data fully removed. They will be placed in `whole_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_outlier_mode: str = \"median\"; # median | stripped | mean\n",
    "\n",
    "# Only operating on listings for sale\n",
    "whole_df = df[df['status'] != \"sold\"];\n",
    "\n",
    "# Street encoded value cannot be reproduced when given new data.\n",
    "# prev_sold_date useless, not a time series problem.\n",
    "whole_df = whole_df.drop(columns=['street', 'status', 'prev_sold_date', 'city', 'brokered_by'], axis=1);\n",
    "\n",
    "# Before Cleaning\n",
    "utils.dataFrameStatus(whole_df)\n",
    "\n",
    "if dataset_outlier_mode == \"mean\":\n",
    "    whole_df['bed'] = whole_df['bed'].fillna(whole_df['bed'].mean());\n",
    "    whole_df['bath'] = whole_df['bath'].fillna(whole_df['bath'].mean());\n",
    "    whole_df['acre_lot'] = whole_df['acre_lot'].fillna(whole_df['acre_lot'].mean());\n",
    "    whole_df['house_size'] = whole_df['house_size'].fillna(whole_df['house_size'].mean());\n",
    "elif dataset_outlier_mode == \"median\":\n",
    "    whole_df['bed'] = whole_df['bed'].fillna(whole_df['bed'].median());\n",
    "    whole_df['bath'] = whole_df['bath'].fillna(whole_df['bath'].median());\n",
    "    whole_df['acre_lot'] = whole_df['acre_lot'].fillna(whole_df['acre_lot'].median());\n",
    "    whole_df['house_size'] = whole_df['house_size'].fillna(whole_df['house_size'].median());\n",
    "\n",
    "whole_df = whole_df.dropna();\n",
    "\n",
    "state_map = {\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Connecticut': 'CT',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New York': 'NY',\n",
    "    'New Hampshire': 'NH',\n",
    "    'Vermont': 'VT',\n",
    "    'Rhode Island': 'RI',\n",
    "    'Wyoming': 'WY',\n",
    "    'Maine': 'ME',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Delaware': 'DE',\n",
    "    'Ohio': 'OH',\n",
    "    'Maryland': 'MD',\n",
    "    'Virginia': 'VA',\n",
    "    'Colorado': 'CO',\n",
    "    'District of Columbia': 'DC',\n",
    "    'North Carolina': 'NC',\n",
    "    'Kentucky': 'KY',\n",
    "    'South Carolina': 'SC',\n",
    "    'Tennessee': 'TN',\n",
    "    'Georgia': 'GA',\n",
    "    'Alabama': 'AL',\n",
    "    'Florida': 'FL',\n",
    "    'Mississippi': 'MS',\n",
    "    'Texas': 'TX',\n",
    "    'Missouri': 'MO',\n",
    "    'Arkansas': 'AR',\n",
    "    'Louisiana': 'LA',\n",
    "    'Indiana': 'IN',\n",
    "    'Illinois': 'IL',\n",
    "    'Michigan': 'MI',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Iowa': 'IA',\n",
    "    'Minnesota': 'MN',\n",
    "    'South Dakota': 'SD',\n",
    "    'Nebraska': 'NE',\n",
    "    'North Dakota': 'ND',\n",
    "    'Montana': 'MT',\n",
    "    'Idaho': 'ID',\n",
    "    'Kansas': 'KS',\n",
    "    'Oklahoma': 'OK',\n",
    "    'New Mexico': 'NM',\n",
    "    'Utah': 'UT',\n",
    "    'Nevada': 'NV',\n",
    "    'Washington': 'WA',\n",
    "    'Oregon': 'OR',\n",
    "    'Arizona': 'AZ',\n",
    "    'California': 'CA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Guam': 'GU',\n",
    "    'Alaska': 'AK'\n",
    "}\n",
    "\n",
    "whole_df['state'] = whole_df['state'].map(state_map);\n",
    "\n",
    "le = LabelEncoder();\n",
    "whole_df['state'] = le.fit_transform(whole_df['state']);\n",
    "\n",
    "# After Cleaning\n",
    "print(\"\\nAfter Cleaning\\n\");\n",
    "utils.dataFrameStatus(whole_df)\n",
    "len(whole_df), whole_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 32;\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col_names = [col for col in self.df.columns if col != \"price\"]\n",
    "        features = torch.tensor(self.df.iloc[index][col_names].values, dtype=torch.float32)\n",
    "        label = torch.tensor(self.df.iloc[index]['price'], dtype=torch.float32)\n",
    "\n",
    "        return features, label;\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(whole_df, test_size=0.2, train_size=0.8);\n",
    "\n",
    "train_dataset = CustomDataset(train_df);\n",
    "test_dataset = CustomDataset(test_df);\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True);\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False);\n",
    "\n",
    "next(iter(train_loader)), next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self, input_features, num_blocks, hidden_units=64, dropout=0.3) -> None:\n",
    "        super().__init__();\n",
    "\n",
    "        self.num_blocks = num_blocks;\n",
    "\n",
    "        self.input = nn.Linear(in_features=input_features, out_features=hidden_units);\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(dropout)\n",
    "        );\n",
    "    \n",
    "        self.classifier = nn.Linear(in_features=hidden_units, out_features=1);\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.input(x);\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = self.block(x);\n",
    "\n",
    "        return self.classifier(x);\n",
    "\n",
    "model = LinearBaseline(hidden_units=256,\n",
    "                       dropout=0.3,\n",
    "                       input_features=len(next(iter(train_loader))[0][0]),\n",
    "                       num_blocks=3).to(DEVICE);\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: LinearBaseline,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optim: torch.optim.Optimizer) -> Tuple[float, float]:\n",
    "\n",
    "    running_loss = 0.0;\n",
    "    running_acc = 0.0;\n",
    "    total_samples = 0;\n",
    "    \n",
    "    for X, y in tqdm(loader):\n",
    "        X: torch.Tensor = X.to(DEVICE);\n",
    "        y: torch.Tensor = y.to(DEVICE);\n",
    "\n",
    "        # Ensure model output matches target shape\n",
    "        outputs: torch.Tensor = model(X).squeeze(-1);\n",
    "\n",
    "        # Calculate loss only if shapes match\n",
    "        if outputs.shape == y.shape:\n",
    "            loss = loss_fn(outputs, y);\n",
    "            \n",
    "            loss.backward();\n",
    "            optim.step();\n",
    "\n",
    "            running_loss += loss.item();\n",
    "            running_acc += utils.batchAccuracy(outputs, y);\n",
    "\n",
    "            total_samples += len(y);\n",
    "\n",
    "    # Return average loss per sample\n",
    "    return running_loss / total_samples if total_samples > 0 else 0.0, running_acc / total_samples if total_samples > 0 else 0.0;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Step\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    model: LinearBaseline,\n",
    "    loader: DataLoader,\n",
    "    loss_fn: nn.Module) -> Tuple[float, float]:\n",
    "    \n",
    "    running_loss = 0.0;\n",
    "    running_acc = 0.0;\n",
    "    total_samples = 0;\n",
    "    \n",
    "    for X, y in tqdm(loader):\n",
    "        X: torch.Tensor = X.to(DEVICE);\n",
    "        y: torch.Tensor = y.to(DEVICE);\n",
    "\n",
    "        # Ensure model output matches target shape\n",
    "        outputs: torch.Tensor = model(X).squeeze(-1);\n",
    "\n",
    "        # Calculate loss only if shapes match\n",
    "        if outputs.shape == y.shape:\n",
    "            loss = loss_fn(outputs, y);\n",
    "            running_loss += loss.item();\n",
    "            running_acc += utils.batchAccuracy(outputs, y);\n",
    "            total_samples += len(y);\n",
    "\n",
    "    # Return average loss per sample\n",
    "    return running_loss / total_samples if total_samples > 0 else 0.0, running_acc / total_samples if total_samples > 0 else 0.0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2);\n",
    "loss_fn = torch.nn.MSELoss();\n",
    "EPOCHS = 25;\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    model.train();\n",
    "    train_loss, train_acc = train_step(\n",
    "        loader=train_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        model=model,\n",
    "        optim=optimizer,\n",
    "    );\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        model.eval();\n",
    "        test_loss, test_acc = test_step(\n",
    "            loader=test_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            model=model,\n",
    "        );\n",
    "\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}%, Test Accuracy: {test_acc:.2f}%\");\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\");\n",
    "    print(f\"Training Loss: {train_loss:.4f}%, Test Loss: {test_loss:.4f}%\");\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(6);\n",
    "torch.onnx.export(model,\n",
    "                  dummy_input,\n",
    "                  \"model.onnx\",\n",
    "                  opset_version=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
